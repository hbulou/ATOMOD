import os
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, CSVLogger
from ATOMOD.ATOMOD_CORRECTED import CustomDataGenerator, UNet, ImageSamplingCallback


# ========================================
# FONCTIONS DE LOSS AM√âLIOR√âES
# ========================================

def weighted_bce_loss(y_true, y_pred):
    """
    Binary Cross Entropy pond√©r√©e
    ‚úÖ CORRECTION : pos_weight r√©duit de 20 √† 5 pour plus de stabilit√©
    """
    epsilon = tf.keras.backend.epsilon()
    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)

    # Poids r√©duit pour √©viter l'instabilit√©
    pos_weight = 5.0  # R√©duit de 20.0 √† 5.0
    
    loss = - (pos_weight * y_true * tf.math.log(y_pred) + 
              (1 - y_true) * tf.math.log(1 - y_pred))
    return tf.reduce_mean(loss)


def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):
    """
    Focal Loss pour g√©rer le d√©s√©quilibre de classes
    Focus sur les exemples difficiles √† classer
    
    Args:
        alpha: Balance entre classes positives/n√©gatives (0.25 = 25% poids sur positives)
        gamma: Focus sur exemples difficiles (2.0 = standard)
    """
    epsilon = tf.keras.backend.epsilon()
    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
    
    # Cross entropy standard
    cross_entropy = - (y_true * tf.math.log(y_pred) + 
                       (1 - y_true) * tf.math.log(1 - y_pred))
    
    # Terme focal : r√©duit la perte sur les exemples bien class√©s
    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
    focal_term = tf.pow(1 - p_t, gamma)
    
    # Poids alpha pour balance positif/n√©gatif
    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)
    
    loss = alpha_factor * focal_term * cross_entropy
    return tf.reduce_mean(loss)


def dice_loss(y_true, y_pred, smooth=1e-6):
    """
    Dice Loss : excellente pour la segmentation
    Mesure le chevauchement entre pr√©diction et v√©rit√© terrain
    """
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])
    
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f)
    
    dice = (2. * intersection + smooth) / (union + smooth)
    return 1 - dice


def combined_loss(y_true, y_pred):
    """
    Combinaison de Focal Loss et Dice Loss
    Recommand√© pour la segmentation avec d√©s√©quilibre
    """
    focal = focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0)
    dice = dice_loss(y_true, y_pred)
    return 0.7 * focal + 0.3 * dice


# ========================================
# CLASSE TRAINER
# ========================================

class ATOMODTrainer:
    """Entra√Æneur pour le mod√®le ATOMOD UNet - VERSION CORRIG√âE"""
    
    def __init__(self, config=None):
        """
        Initialise le trainer avec une configuration.
        
        Args:
            config (dict, optional): Dictionnaire de configuration personnalis√©e
        """
        # Configuration par d√©faut
        self.config = {
            'batch_size': 64,
            'epochs': 200000,
            'height': 64,
            'width': 64,
            'composition': ['Rh', 'Ir'],
            'nz': 10,
            'n_train_images': 2048,
            'n_val_images': 2048,
            'restart': False,
            'checkpoint_path': 'unet_atomod_trained_last.keras',
            'initial_epoch': 0,
            'learning_rate': 1e-4,
            'data_root': 'data/train',
            'output_dir': 'model/intermediate',
            'logs_dir': 'model/logs',
            'checkpoint_dir': 'model/checkpoints',
            'save_best_only': False,
            'early_stopping_patience': 2000,
            'checkpoint_freq': 100,
            'debug_mode': False,  # ‚úÖ AJOUT√â
            'loss_function': 'combined'  # ‚úÖ AJOUT√â : 'weighted_bce', 'focal', 'dice', 'combined'
        }
        
        # Mise √† jour avec la config personnalis√©e si fournie
        if config:
            self.config.update(config)
        
        # Cr√©ation des dossiers n√©cessaires
        os.makedirs(self.config['checkpoint_dir'], exist_ok=True)
        os.makedirs(self.config['logs_dir'], exist_ok=True)
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
        self.model = None
        self.device = None
        self.strategy = None
    
    def setup_gpu(self):
        """Configure et d√©tecte les GPU disponibles avec optimisations."""
        gpus = tf.config.list_physical_devices('GPU')
        
        if gpus:
            print(f"‚úÖ {len(gpus)} GPU(s) d√©tect√©(s):")
            for gpu in gpus:
                print(f"   - {gpu}")
            self.device = "cuda"
            
            # OPTIMISATION 1: Croissance m√©moire dynamique
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print("   ‚úì Memory growth activ√©")
            except RuntimeError as e:
                print(f"   ‚ö†Ô∏è Erreur lors de la configuration GPU: {e}")
            
            # OPTIMISATION 2: XLA (Accelerated Linear Algebra)
            tf.config.optimizer.set_jit(True)
            print("   ‚úì XLA JIT compilation activ√©e")
            
            # OPTIMISATION 3: Parallelisme
            tf.config.threading.set_intra_op_parallelism_threads(8)
            tf.config.threading.set_inter_op_parallelism_threads(4)
            print("   ‚úì Threading optimis√© (intra=8, inter=4)")
            
        else:
            print("‚ö†Ô∏è Aucun GPU d√©tect√©, utilisation du CPU")
            self.device = "cpu"
        
        return self.device
    
    def create_data_generators(self):
        """Cr√©e les g√©n√©rateurs de donn√©es pour l'entra√Ænement et la validation."""
        batch_size = self.config['batch_size']
        n_train = self.config['n_train_images']
        n_val = self.config['n_val_images']
        
        # G√©n√©ration des IDs
        train_IDs = [f'img_{i:04d}' for i in range(n_train)]
        val_IDs = [f'img_{i:04d}' for i in range(n_train, n_train + n_val)]
        
        print(f"\nüìä CONFIGURATION DES DONN√âES:")
        print(f"   Entra√Ænement: {len(train_IDs)} images (img_0000 √† img_{n_train-1:04d})")
        print(f"   Validation: {len(val_IDs)} images (img_{n_train:04d} √† img_{n_train+n_val-1:04d})")
        print(f"   Batch size: {batch_size}")
        print(f"   Steps/epoch train: {len(train_IDs) // batch_size}")
        print(f"   Steps/epoch val: {len(val_IDs) // batch_size}")
        
        # ‚úÖ G√©n√©rateur avec debug_mode
        train_generator = CustomDataGenerator(
            train_IDs,
            self.config['data_root'],
            (self.config['height'], self.config['width']),
            batch_size,
            shuffle=True,
            composition=self.config['composition'],
            nz=self.config['nz'],
            debug_mode=self.config['debug_mode']  # ‚úÖ AJOUT√â
        )
        
        val_generator = CustomDataGenerator(
            val_IDs,
            self.config['data_root'],
            (self.config['height'], self.config['width']),
            batch_size,
            shuffle=False,
            composition=self.config['composition'],
            nz=self.config['nz'],
            debug_mode=False  # Pas de debug en validation
        )
        
        return train_generator, val_generator, val_IDs
    
    def build_model(self):
        """Construit ou charge le mod√®le UNet."""
        # Configuration multi-GPU
        self.strategy = tf.distribute.MirroredStrategy()
        print(f"\nüîß CONFIGURATION MOD√àLE:")
        print(f"   Distribution sur {self.strategy.num_replicas_in_sync} GPU(s)")
        
        with self.strategy.scope():
            # Calcul du nombre de canaux de sortie
            output_channels = len(self.config['composition']) * self.config['nz']
            print(f"   Output channels: {output_channels} ({len(self.config['composition'])} esp√®ces √ó {self.config['nz']} couches)")
            
            # Chargement ou cr√©ation du mod√®le
            if self.config['restart'] and os.path.exists(self.config['checkpoint_path']):
                print(f"   üì• Chargement du mod√®le depuis {self.config['checkpoint_path']}")
                self.model = load_model(self.config['checkpoint_path'], compile=False)
            else:
                print("   üÜï Cr√©ation d'un nouveau mod√®le UNet (avec skip connections)")
                self.model = UNet(
                    self.config['height'],
                    self.config['width'],
                    output_channels
                )
            
            # ‚úÖ S√©lection de la fonction de loss
            loss_functions = {
                'weighted_bce': weighted_bce_loss,
                'focal': focal_loss,
                'dice': dice_loss,
                'combined': combined_loss
            }
            
            selected_loss = loss_functions.get(
                self.config['loss_function'], 
                combined_loss
            )
            
            print(f"   Loss function: {self.config['loss_function']}")
            
            # Compilation du mod√®le
            self.model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=self.config['learning_rate'],
                    clipnorm=1.0
                ),
                loss=selected_loss,
                metrics=[
                    tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy'),
                    tf.keras.metrics.Precision(name='precision'),
                    tf.keras.metrics.Recall(name='recall')
                ]
            )
        
        print("\nüìã ARCHITECTURE DU MOD√àLE:")
        self.model.summary()
        return self.model
    
    def create_callbacks(self, val_generator, val_IDs):
        """Cr√©e la liste des callbacks pour l'entra√Ænement."""
        callbacks = []
        
        # CALLBACK 1: Sauvegarde r√©guli√®re
        checkpoint_callback = ModelCheckpoint(
            filepath=os.path.join(
                self.config['checkpoint_dir'],
                'unet_epoch_{epoch:06d}.keras'
            ),
            save_best_only=False,
            save_freq=self.config['checkpoint_freq'],
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        # CALLBACK 2: Sauvegarde du meilleur mod√®le
        best_checkpoint = ModelCheckpoint(
            filepath=os.path.join(self.config['checkpoint_dir'], 'best_model.keras'),
            save_best_only=True,
            monitor='val_loss',
            mode='min',
            verbose=1
        )
        callbacks.append(best_checkpoint)
        
        # CALLBACK 3: TensorBoard
        tensorboard_callback = TensorBoard(
            log_dir=self.config['logs_dir'],
            histogram_freq=0,
            write_graph=False,
            update_freq='epoch',
            profile_batch=0
        )
        callbacks.append(tensorboard_callback)
        
        # CALLBACK 4: CSV Logger
        csv_logger = CSVLogger(
            filename=os.path.join(self.config['logs_dir'], 'training_history.csv'),
            separator=',',
            append=True
        )
        callbacks.append(csv_logger)
        
        # CALLBACK 5: Reduce LR on Plateau
        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=500,
            min_lr=1e-7,
            verbose=1
        )
        callbacks.append(reduce_lr)
        
        # CALLBACK 6: Early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=self.config['early_stopping_patience'],
            restore_best_weights=True,
            verbose=1
        )
        callbacks.append(early_stopping)
        
        # CALLBACK 7: Visualisation personnalis√©e
        try:
            sample_batch_input, _ = val_generator[0]
            sample_input_for_callback = sample_batch_input[0:1]
            
            image_sampling_callback = ImageSamplingCallback(
                sample_input_image=sample_input_for_callback,
                class_channel_index=0,
                val_IDs=val_IDs[:5],  # 5 images de validation
                nz=self.config['nz'],
                composition=self.config['composition'],
                output_dir=self.config['output_dir'],
                H=self.config['height'],
                W=self.config['width'],
                freq_img_save=10  # Sauvegarde tous les 10 epochs
            )
            callbacks.append(image_sampling_callback)
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de cr√©er ImageSamplingCallback: {e}")
        
        return callbacks
    
    def evaluate_initial_performance(self, val_generator):
        """√âvalue les performances initiales du mod√®le."""
        print("\n" + "="*60)
        print("üìä √âVALUATION PR√â-ENTRA√éNEMENT")
        print("="*60)
        
        try:
            # Test sur un petit √©chantillon
            sample_X, sample_Y = val_generator[0]
            
            # Statistiques des donn√©es
            print(f"\nüìà STATISTIQUES DES DONN√âES:")
            print(f"   X (entr√©e) - Min: {sample_X.min():.4f}, Max: {sample_X.max():.4f}, Mean: {sample_X.mean():.4f}")
            print(f"   Y (masque) - Min: {sample_Y.min():.4f}, Max: {sample_Y.max():.4f}, Mean: {sample_Y.mean():.4f}")
            print(f"   Shape X: {sample_X.shape}, Shape Y: {sample_Y.shape}")
            
            # Pr√©diction initiale
            pred = self.model.predict(sample_X[:1], verbose=0)
            print(f"\nüîÆ PR√âDICTION INITIALE:")
            print(f"   Min: {pred.min():.4f}, Max: {pred.max():.4f}, Mean: {pred.mean():.4f}")
            print(f"   Shape: {pred.shape}")
            
            # √âvaluation compl√®te
            initial_metrics = self.model.evaluate(
                val_generator,
                steps=min(5, len(val_generator)),
                verbose=1
            )
            
            metric_names = ['Loss', 'Accuracy', 'Precision', 'Recall']
            print(f"\n‚úÖ M√âTRIQUES INITIALES:")
            for name, value in zip(metric_names, initial_metrics):
                print(f"   {name}: {value:.4f}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'√©valuation initiale: {e}")
        
        print("="*60 + "\n")
    
    def train(self):
        """Lance l'entra√Ænement complet du mod√®le."""
        print("\n" + "="*60)
        print("üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT ATOMOD - VERSION CORRIG√âE")
        print("="*60 + "\n")
        
        # 1. Configuration GPU
        self.setup_gpu()
        
        # 2. Cr√©ation des g√©n√©rateurs de donn√©es
        train_gen, val_gen, val_IDs = self.create_data_generators()
        
        # 3. Construction du mod√®le
        self.build_model()
        
        # 4. √âvaluation initiale
        self.evaluate_initial_performance(val_gen)
        
        # 5. Cr√©ation des callbacks
        callbacks = self.create_callbacks(val_gen, val_IDs)
        
        # 6. Entra√Ænement
        print("üèãÔ∏è D√âBUT DE L'ENTRA√éNEMENT...")
        print(f"   - Sauvegarde tous les {self.config['checkpoint_freq']} epochs")
        print(f"   - Early stopping apr√®s {self.config['early_stopping_patience']} epochs sans am√©lioration")
        print(f"   - Loss function: {self.config['loss_function']}\n")
        
        history = self.model.fit(
            train_gen,
            steps_per_epoch=len(train_gen),
            epochs=self.config['epochs'],
            initial_epoch=self.config['initial_epoch'],
            validation_data=val_gen,
            validation_steps=len(val_gen),
            callbacks=callbacks,
            verbose=1
        )
        
        # 7. Sauvegarde finale
        final_model_path = os.path.join(self.config['checkpoint_dir'], 'unet_atomod_trained_final.keras')
        self.model.save(final_model_path)
        
        print(f"\n‚úÖ ENTRA√éNEMENT TERMIN√â!")
        print(f"üíæ Mod√®le sauvegard√©: {final_model_path}")
        print(f"üìä Historique CSV: {os.path.join(self.config['logs_dir'], 'training_history.csv')}")
        
        return history


# ========================================
# FONCTION MAIN
# ========================================

def main():
    """Point d'entr√©e principal du programme."""
    
    # Configuration recommand√©e pour d√©marrer
    batch_size = 64  # Commencer avec 64, augmenter si GPU sous-utilis√©
    save_dir = "model_corrected_64x64_" + str(batch_size)
    
    config = {
        'batch_size': batch_size,
        'epochs': 200000,
        'height': 64,
        'width': 64,
        'composition': ['Rh', 'Ir'],
        'nz': 10,
        'n_train_images': 2048,
        'n_val_images': 2048,
        'restart': False,
        'checkpoint_path': 'unet_atomod_trained.keras',
        'initial_epoch': 0,
        'learning_rate': 1e-4,
        'data_root': 'data/train',
        'output_dir': save_dir + '/intermediate',
        'logs_dir': save_dir + '/logs',
        'checkpoint_dir': save_dir + '/checkpoints',
        'save_best_only': False,
        'early_stopping_patience': 2000,
        'checkpoint_freq': 100,
        'debug_mode': True,  # ‚úÖ ACTIVER pour le premier batch
        'loss_function': 'combined'  # ‚úÖ 'combined' recommand√© (focal + dice)
    }
    
    # ‚ö†Ô∏è IMPORTANT : Apr√®s le premier batch, d√©sactiver debug_mode
    # config['debug_mode'] = False
    
    print("\nüìù CONFIGURATION:")
    print(f"   Batch size: {config['batch_size']}")
    print(f"   Loss function: {config['loss_function']}")
    print(f"   Debug mode: {config['debug_mode']}")
    print(f"   Save directory: {save_dir}")
    
    # Cr√©ation et lancement de l'entra√Æneur
    trainer = ATOMODTrainer(config=config)
    history = trainer.train()
    
    print("\nüéâ PROCESSUS TERMIN√â!")


if __name__ == "__main__":
    main()
